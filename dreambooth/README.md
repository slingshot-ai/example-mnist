# Dreambooth ðŸª„


| 1                      | 2                      | 3                      |
|------------------------|------------------------|------------------------|
| ![Demo image 1][demo1] | ![Demo image 2][demo2] | ![Demo image 3][demo3] |

[demo1]: img/img1.png
[demo2]: img/img2.png
[demo3]: img/img3.png

There are several text-to-image models like Stable Diffusion and Imagen available that allow you to produce images of
objects or fictional characters in completely novel settings. It is as simple as asking the model to generate an image
of "Darth Vader baking for the Cookie Monster". But what if you wanted to insert yourself in the image? What if Darth
Vader were baking for you? What if you wanted a picture of you and your friends taking a selfie with Darth Vader? Stable
Diffusion and similar models have no concept of you or your friends, so simply inserting your names wouldn't get the job
done. If only there were a way to _teach_ these models the concept of _you_...

Dreambooth is a method that allows you to do exactly that!

It is a training regime used to personalize text-to-image models. In this project, we will use
[Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) to produce personalized images from
descriptive texts.

## 0. Set up a project on Slingshot

1. Create a new project on [Slingshot](https://dev.slingshot.xyz/)
1. Set this Slingshot project as active on your local machine:

```bash
$ slingshot use
Select the project you want to work on:
...
> dreambooth-1234
...
```

## 1. Prepare and upload your images for training

### 1.1 Preparing images:

1. Clone this repo
2. Select 30 - 50 images of you in various outfits, backgrounds, and expressions and place them into a folder anywhere
   on your local machine (`<path/to/images>`).

> [!IMPORTANT]
> The images you provide should be approximately square. Non-square images will be cropped to a square
> either by center-cropping or random cropping, which might remove the subject of the image.

```bash
$ ls <path/to/images>
0H5A5898.JPG         20211003_134450.jpg      IMG-20210304-WA0008.jpg  IMG-20210627-WA0028.jpg  IMG-20220923-WA0006.jpg  IMG-20230622-WA0023.jpg  IMG-20230708-WA0024.jpg
...
20201129_223246.jpg  IMG-20210214-WA0015.jpg  IMG-20210507-WA0011.jpg  IMG-20220616-WA0038.jpg  IMG-20221230-WA0015.jpg  IMG-20230708-WA0020.jpg
```

### 1.2 Uploading your images to Slingshot:

In order to use your images for training, we have to make them accessible in the environment Slingshot uses to run your
training script. We can use **Artifacts** for this. A Slingshot Artifact is simply a file or folder that you can mount
onto Runs, Deployments, or Apps to make these files accessible to your code. Let's create one for our images.

```bash
$ slingshot artifact upload --name source-images <path/to/images>
```

Notice that we've associated a name to our artifact. This makes it easy to reference this artifact in other places and
attach it wherever we want. If you take a peek inside the `slingshot.yaml` file, you'll see a section that mounts this
artifact (with name `source-images`) to our training run.

```bash
- mode: DOWNLOAD
  path: /mnt/images
  selector:
    name: source-images
```

### 1.3. (Optional) Prepare generic images

[Prior preservation](https://huggingface.co/docs/diffusers/training/dreambooth#finetuning-with-priorpreserving-loss) is a technique that prevents the model from overfitting to the target entity.
It does so by using generic images generated by the base model and ensures that the finetuned model can generate those
images as well. Many people have found it to be useful, especially when finetuning on human faces.

To do this, we need to sample generic images from the base model. This is a time-consuming process, but thankfully the
community has already done this for us. We can use the generic images provided by
[this repo](https://github.com/jbpacker/dreambooth_class_images/tree/main) which has images for the following classes:

- "a photo of a person"
- "photo of a cat"
- "photo of a landscape"
- "dune landscape"

In our case, we will only use "a photo of a person". We can save those images into an Artifact using a **Run**:

```bash
$ slingshot run start download-generic-images
```

If you take a peek inside the `slingshot.yaml` file, you'll see that it simply runs a bash script
[`entrypoints/download_generic_images.sh`](entrypoints/download_generic_images.sh) and saves the images into an Artifact
with name `generic-images`.

> TODO: What if I want to use something other than "a photo of a person"?

## 2. Push code to your Slingshot environment

You'll need a training script to train the Dreambooth model. We've provided some starter code (`entrypoint/train.py`)
that takes Stable Diffusion 1.5 and fine-tunes it with your provided images. Push this code into your Slingshot project
by `slingshot push`:

```bash
$ slingshot push
Pushed new source code 'teeming-magic-42', view in browser at https://dev.slingshot.xyz/project/dreambooth-1234/code/14c353232b
```

Now, you are all set to train and generate your dream images!

## 3. Train your model

### 3.1 Add WandB API key

We use [WandB](tps://wandb.ai/) to track the training process, you'll need to add your API key to your project.
You can find it at [https://wandb.ai/authorize](https://wandb.ai/authorize). Add your API key as a **Secret**:

```bash
$ slingshot secret wandb <your-wandb-api-key>
```

### 3.2. Tweak the hyperparameters and train!

We found the default hyperparameters in `slinghot.yaml` to work well for most people. However, feel free to experiment
their values. In particular, you might want to change the following:

- `wandb_project`: The default is `dreambooth`. You can change it to something else if you want to track your training
  runs with a different WandB project.
- `max_train_steps`: The default is `1200`. If you find that the model is not over-fitting, you can increase this number
  to `1500` or `2000`. If it is over-fitting, maybe reduce it to around `800` steps.
- `learning_rate`: The default is `3e-6`. You can vary it between `1e-6` and `5e-6` depending on how much the model is
  over- or under-fitting.
- `train_text_encoder`: The default is `True`. Setting it to `False` can help stablize the training process,
  particularly useful if you are not using prior preservation.

You can execute your training code by creating a **Run**. This can be done through the UI or the CLI.

**Using the CLI**

You can see what values are being used in the `slingshot.yaml` file. These hyperparameters are defined within the
`config_variables` section of a Run. If you do change them, remember to apply the changes:

```bash
$ slingshot apply
```

Once your parameters are set, kick off your training run using:

```bash
$ slingshot run start train-dreambooth
```

You can monitor the progress by visiting the WandB dashboard.

**Using the frontend**

On the front end, you can modify the hyperparameters by going to the **Create Run** section of the **Runs** tab, then
select the **train-dreambooth** template from the dropdown.

Note that the default hyperparameter values are read from the `slingshot.yaml` file. Changes made in the UI will only
apply to that specific run. If you wish to save your changes permanently, you can select the checkbox **Update
Template**.

Once your parameters are set, kick off your training run using the **Start Run** button at the bottom of the page.

## 4. Generate Images

### 4.1. Start a Deployment

To host the finetuned model, we can start a **Deployment**. This will host the model as an API server and allow you to
generate images from it:

```bash
$ slingshot inference start image-generation
```

This will mount the latest Artifact with name `trained-model`. You can also specify a specific model version by using
specifying the `tag` in the `slingshot.yaml` file:

```yaml
deployments:
  - name: image-generation
    ...
    mounts:
      - mode: DOWNLOAD
        path: /mnt/model
        selector:
          name: trained-model
          tag: happy-dinosaur-1  # <-- Add this line
    cmd: python entrypoints/inference.py
```

### 4.2. Connect to the Deployment with Gradio

A **Deployment** is meant to be a generic API server. To make it easier to interact with the model, we can use
[Gradio](https://gradio.app/), a tool for quickly creating UIs for your models.

We can start the Gradio **App** by:

```bash
$ slingshot app start demo
```

which will give you a link to the app and you can start generating new images!

### 4.3. Inference tips

- You should use prompts with as detailed descriptions as possible. The example prompts in the Gradio app are good
  starting points.
- Picking the right checkpoint is important. Early checkpoints tend to give more diverse results, but it also captures
  the concept of the target less accurately. Explore for yourself!

Finally, here's Darth Vader giving me some cookies!

| 1                      | 2                      |
|------------------------|------------------------|
| ![Demo image 4][demo4] | ![Demo image 5][demo5] |

[demo4]: img/img4.png
[demo5]: img/img5.png
